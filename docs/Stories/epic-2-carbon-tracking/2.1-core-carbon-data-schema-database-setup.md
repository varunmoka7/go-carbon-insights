---
id: 2.1-core-carbon-data-schema-database-setup
title: Core Carbon Data Schema & Database Setup
epic: Epic 2 â€“ Carbon Tracking & Public Platform + Real Data Integration
phase: 2.1
status: planned
---

## Background
The foundation of the carbon tracking platform requires a robust, scalable database schema that can handle 100k+ companies with comprehensive ESG data across multiple scopes, time periods, and data sources. This story establishes the core data architecture that will support all carbon tracking, benchmarking, and analytics features.

## User Story
**As a** Platform Developer,
**I want** a comprehensive database schema for carbon emissions and ESG data with proper indexing and relationships,
**so that** the platform can efficiently store, query, and analyze emissions data for 100k+ companies while maintaining data integrity and performance.

## Acceptance Criteria

1. **Core Company Schema**
   - Companies table with unique identifiers, ISIN/ticker symbols, and hierarchical relationships
   - Industry classification supporting GICS, NAICS, and custom sector mappings
   - Company metadata including size, geography, founding date, and operational status
   - Support for company mergers, acquisitions, and name changes with historical tracking

2. **Emissions Data Schema**
   - Scope 1, 2, and 3 emissions tables with standardized units (tCO2e)
   - Time-series data structure supporting monthly, quarterly, and annual reporting
   - Data source attribution with credibility scores and verification status
   - Support for estimated, reported, and verified emission values

3. **ESG Metrics Schema**
   - Financial performance data (revenue, EBITDA, market cap) for intensity calculations
   - Energy consumption data (renewable %, total consumption, efficiency metrics)
   - Water usage, waste generation, and circular economy indicators
   - Social metrics (employee count, diversity, safety records)
   - Governance scores and compliance framework adherence

4. **Data Quality & Metadata**
   - Data lineage tracking from source to aggregation
   - Quality scores, completeness metrics, and confidence intervals
   - Update timestamps, data freshness indicators, and validation status
   - Audit trail for all data modifications and corrections

5. **Performance Optimization**
   - Partitioned tables by date for historical data management
   - Optimized indexes for common query patterns (company, industry, time range)
   - Materialized views for aggregated industry benchmarks
   - Connection pooling and query optimization for 100k company scale

6. **Security & Compliance**
   - Row-level security (RLS) policies for data access control
   - Encryption at rest for sensitive financial and operational data
   - GDPR compliance for any personal data elements
   - Audit logging for all database operations

## Technical Implementation Tasks

### Database Schema Design
- [ ] Create `companies` table with hierarchical relationships and metadata
- [ ] Design `industries` and `sectors` tables with multiple classification systems
- [ ] Implement `scope1_emissions`, `scope2_emissions`, `scope3_emissions` tables
- [ ] Create `financial_data` table for revenue, market cap, and performance metrics
- [ ] Design `energy_data` table for consumption, renewable %, and efficiency
- [ ] Create `esg_metrics` table for comprehensive sustainability indicators
- [ ] Implement `data_sources` table for attribution and credibility tracking
- [ ] Design `data_quality` table for completeness and confidence scoring

### Indexing & Performance
- [ ] Create composite indexes for company + date range queries
- [ ] Implement partial indexes for verified vs estimated data
- [ ] Design covering indexes for dashboard aggregation queries
- [ ] Create GIN indexes for text search and filtering
- [ ] Implement table partitioning by date for historical data
- [ ] Create materialized views for industry benchmarks and aggregations

### Data Integrity & Constraints
- [ ] Implement foreign key constraints maintaining referential integrity
- [ ] Create check constraints for data validation (positive emissions, valid ranges)
- [ ] Design triggers for automatic data quality score calculation
- [ ] Implement soft delete functionality with historical preservation
- [ ] Create audit triggers for change tracking and compliance

### Migration Scripts
- [ ] Write initial schema creation migration with proper ordering
- [ ] Create data seeding scripts for industries and sectors
- [ ] Implement demo data generation for development and testing
- [ ] Design rollback procedures for schema changes
- [ ] Create database backup and restore procedures

## Database Schema Requirements

### Core Tables Structure

```sql
-- Companies table with hierarchical support
CREATE TABLE companies (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    legal_name VARCHAR(255),
    isin VARCHAR(12) UNIQUE,
    ticker VARCHAR(10),
    country_code VARCHAR(3),
    founded_date DATE,
    employee_count INTEGER,
    market_cap DECIMAL(15,2),
    parent_company_id UUID REFERENCES companies(id),
    industry_id UUID NOT NULL,
    sector_id UUID NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- Industry classification with multiple standards
CREATE TABLE industries (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    gics_code VARCHAR(8),
    naics_code VARCHAR(6),
    description TEXT,
    sector_id UUID NOT NULL
);

-- Emissions data with time-series structure
CREATE TABLE scope1_emissions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    company_id UUID NOT NULL REFERENCES companies(id),
    reporting_period DATE NOT NULL,
    emissions_tco2e DECIMAL(12,2) NOT NULL,
    data_source_id UUID NOT NULL,
    verification_status VARCHAR(20) DEFAULT 'unverified',
    estimation_method VARCHAR(50),
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
) PARTITION BY RANGE (reporting_period);

-- ESG metrics comprehensive table
CREATE TABLE esg_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    company_id UUID NOT NULL REFERENCES companies(id),
    reporting_period DATE NOT NULL,
    revenue DECIMAL(15,2),
    energy_consumption_mwh DECIMAL(12,2),
    renewable_energy_percentage DECIMAL(5,2),
    water_consumption_m3 DECIMAL(12,2),
    waste_generated_tonnes DECIMAL(10,2),
    data_quality_score DECIMAL(3,2) DEFAULT 0.0,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

## Integration Points

### Existing Features
- **Authentication System**: Leverage user roles for data access permissions
- **Admin Dashboard**: Connect to existing admin interface for data management
- **Search Infrastructure**: Build upon existing search service architecture
- **API Framework**: Extend current API structure for data endpoints

### Future Integrations
- **Data Import System**: Foundation for Story 2.9 enterprise import capabilities
- **Real-time Updates**: Schema support for Story 2.7 auto-refresh features
- **Benchmarking Tools**: Data structure enabling Story 2.6 comparison features
- **API Access**: Schema foundation for Story 2.8 public API development

## Performance Considerations (100k Company Scale)

### Query Optimization
- Materialized views refreshed nightly for dashboard aggregations
- Partial indexes on frequently filtered columns (verification_status, country)
- Connection pooling with 50-100 concurrent connections
- Query result caching for repeated aggregation requests

### Storage Management
- Table partitioning by year for emissions time-series data
- Archive older data (>5 years) to separate tablespace
- Compression for historical data reducing storage by 60-70%
- Regular VACUUM and ANALYZE operations for performance maintenance

### Scalability Architecture
- Read replicas for dashboard and public API queries
- Write optimization through batch insertions and transactions
- Horizontal scaling preparation with company-based sharding keys
- Monitoring queries for performance regression detection

## Security Considerations

### Data Protection
- Row-level security policies restricting access by user role
- Encryption at rest for all sensitive financial and operational data
- Column-level encryption for personally identifiable information
- Secure key management through Supabase vault integration

### Access Control
- Role-based permissions (public_read, admin_read, admin_write)
- API key authentication for external data access
- Rate limiting on query-intensive operations
- Audit logging for all data access and modifications

## Dev Notes

### Architecture Context
- Built on Supabase PostgreSQL with automatic backups and scaling
- Leverages Supabase RLS for security without application-layer complexity
- Designed for eventual microservice extraction with clear domain boundaries
- Compatible with existing React + TypeScript frontend architecture

### Technology Stack
- **Database**: PostgreSQL 14+ with Supabase extensions
- **ORM**: Supabase client with TypeScript type generation
- **Migrations**: Supabase CLI migration management
- **Monitoring**: Built-in Supabase dashboard + custom metrics
- **Backup**: Automated daily backups with point-in-time recovery

### Development Workflow
- Schema changes through version-controlled migration files
- Type generation for TypeScript frontend integration
- Seed data scripts for consistent development environments
- Database testing with isolated test schemas

### Testing Strategy
- Unit tests for data validation functions and triggers
- Integration tests for complex queries and aggregations
- Performance tests with simulated 100k company dataset
- Data integrity tests for referential constraints and business rules

## Change Log

| Date       | Version | Description                              | Author |
| :--------- | :------ | :--------------------------------------- | :----- |
| 2025-07-23 | 1.0     | Initial story creation for Epic 2       | Claude |

## Dev Agent Record
*This section will be populated by the development agent.*
- **Agent Model Used:** _TBD_
- **Debug Log References:** _N/A_
- **Completion Notes List:** _N/A_
- **File List:** _N/A_

## QA Results
*This section will be populated during quality assurance review.*